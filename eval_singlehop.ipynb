{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Tuple, Union, Callable, Literal\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Evaluator:\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_answer(cls, s):\n",
    "        def remove_articles(text):\n",
    "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "        def white_space_fix(text):\n",
    "            return ' '.join(text.split())\n",
    "        def remove_punc(text):\n",
    "            exclude = set(string.punctuation)\n",
    "            return ''.join(ch for ch in text if ch not in exclude)\n",
    "        def lower(text):\n",
    "            return text.lower()\n",
    "        if not isinstance(s, str):\n",
    "            return \"\"\n",
    "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "    @classmethod\n",
    "    def exact_match_score(\n",
    "        cls,\n",
    "        prediction: str,\n",
    "        ground_truth: Union[str, List[str]],\n",
    "    ):\n",
    "        if not prediction:\n",
    "            return {'correct': 0, 'incorrect': 1}\n",
    "        ground_truths = {ground_truth} if isinstance(ground_truth, str) else set(ground_truth)\n",
    "\n",
    "        correct = np.max([int(cls.normalize_answer(prediction) == cls.normalize_answer(gt)) for gt in ground_truths])\n",
    "        return {'correct': correct, 'incorrect': 1 - correct}\n",
    "\n",
    "    @classmethod\n",
    "    def f1_score(\n",
    "        cls,\n",
    "        prediction: str,\n",
    "        ground_truth: Union[str, List[str]],\n",
    "    ):\n",
    "        final_metric = {'f1': 0, 'precision': 0, 'recall': 0}\n",
    "        \n",
    "        if not prediction:\n",
    "            return final_metric\n",
    "        ground_truths = {ground_truth} if isinstance(ground_truth, str) else set(ground_truth)\n",
    "            \n",
    "        for ground_truth in ground_truths:\n",
    "            normalized_prediction = cls.normalize_answer(prediction)\n",
    "            normalized_ground_truth = cls.normalize_answer(ground_truth)\n",
    "            if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "                continue\n",
    "            if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "                continue\n",
    "            prediction_tokens = normalized_prediction.split()\n",
    "            ground_truth_tokens = normalized_ground_truth.split()\n",
    "            common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "            num_same = sum(common.values())\n",
    "            if num_same == 0:\n",
    "                continue\n",
    "\n",
    "            precision = 1.0 * num_same / len(prediction_tokens)\n",
    "            recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "            for k in ['f1', 'precision', 'recall']:\n",
    "                final_metric[k] = max(eval(k), final_metric[k])\n",
    "        return final_metric\n",
    "    \n",
    "    def eval_answer(self, results_df, answer_col=\"Final Answer\"):\n",
    "        # for datasets don't have answer_ids, aliases\n",
    "        em_list = []\n",
    "        f1_list = []\n",
    "        for i, row in results_df.iterrows():\n",
    "            prediction = row[answer_col]\n",
    "            ground_truth = row['ground_truth']\n",
    "            em_list.append(self.exact_match_score(prediction, ground_truth)['correct'])\n",
    "            f1_list.append(self.f1_score(prediction, ground_truth)['f1'])\n",
    "        print(f\"EM: {sum(em_list)/len(em_list):4f}\\t F1: {sum(f1_list)/len(f1_list):4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_eval(pred_list, ground_truths):\n",
    "    evaluator = Evaluator()\n",
    "    em_list = []\n",
    "    f1_list = []\n",
    "    for prediction, ground_truth in zip(pred_list, ground_truths):\n",
    "        em_list.append(evaluator.exact_match_score(prediction, ground_truth)['correct'])\n",
    "        f1_list.append(evaluator.f1_score(prediction, ground_truth)['f1'])\n",
    "    print(f\"EM: {sum(em_list)/len(em_list):4f}\\t F1: {sum(f1_list)/len(f1_list):4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name: Literal['nq', 'tq', 'sq'] = 'nq'\n",
    "\n",
    "raw_data = pd.read_json(f\"./data/singlehop_data/processed_{dataset_name}.json\")\n",
    "ground_truth_list = raw_data['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_results = pd.read_json(\"outputs/nq_llama2_7b/direct.jsonl\", lines=True) # replace with your output file\n",
    "rag_results = pd.read_json(\"outputs/nq_llama2_7b/rag.jsonl\", lines=True) # replace with your output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ground_truth_list) == len(direct_results)\n",
    "assert 10 * len(ground_truth_list) == len(rag_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = -6.0\n",
    "answer_list = []\n",
    "for i, direct_res in direct_results.iterrows():\n",
    "    direct_eigen_score = direct_res['eigen_score']\n",
    "    if direct_eigen_score < THRESHOLD:\n",
    "        answer_list.append(direct_res['answer'])\n",
    "    else:\n",
    "        rag_batch = rag_results.iloc[10*i : 10*i+10]\n",
    "        best_answer = rag_batch.loc[rag_batch['eigen_score'].idxmin()]['answer']\n",
    "        answer_list.append(best_answer)\n",
    "my_eval(answer_list, ground_truth_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seakr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
